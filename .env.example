# Voxtral STT API - Environment Configuration
#
# This file is a template. Use one of:
# - Copy to `.env` and export variables from it in your shell, or
# - Export variables directly in your environment.
#
# Do not commit `.env` (it may contain secrets).

# -----------------------------------------------------------------------------
# Required
# -----------------------------------------------------------------------------

# API key required by all WebSocket clients.
# Clients can pass it as:
# - Query parameter:  ?api_key=...
# - Header:           X-API-Key: ...
VOXTRAL_API_KEY=secret_token

# -----------------------------------------------------------------------------
# Recommended
# -----------------------------------------------------------------------------

# Hugging Face token for model downloads (optional but recommended to avoid
# rate limits).
# HF_TOKEN=hf_...

# -----------------------------------------------------------------------------
# Server Bind / Logging
# -----------------------------------------------------------------------------

# Host/port used by the launcher scripts (scripts/main.sh).
SERVER_BIND_HOST=0.0.0.0
SERVER_PORT=8000

# Logging level for the Python server.
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# Model
# -----------------------------------------------------------------------------

# Voxtral Realtime model id (Hugging Face repo).
VOXTRAL_MODEL_ID=mistralai/Voxtral-Mini-4B-Realtime-2602

# Optional: the model name exposed in the realtime protocol ("model" field).
# Defaults to VOXTRAL_MODEL_ID if not set.
# VOXTRAL_SERVED_MODEL_NAME=mistralai/Voxtral-Mini-4B-Realtime-2602

# Intentional transcription delay (ms). Must be a multiple of 80 between 80 and 2400.
VOXTRAL_TRANSCRIPTION_DELAY_MS=400

# Writable local snapshot directory. The server patches `tekken.json` here.
VOXTRAL_MODEL_DIR=models/voxtral

# Tekken config filename inside the snapshot directory.
VOXTRAL_TEKKEN_FILENAME=tekken.json

# -----------------------------------------------------------------------------
# Connection Limits / Lifecycle
# -----------------------------------------------------------------------------

# Maximum concurrent WebSocket connections.
# Default: auto (uses tuned VLLM_MAX_NUM_SEQS).
# Set explicitly to cap connections.
# MAX_CONCURRENT_CONNECTIONS=100

# Idle close (seconds) and watchdog tick interval (seconds).
# Set WS_IDLE_TIMEOUT_S=0 to disable idle close.
WS_IDLE_TIMEOUT_S=150
WS_WATCHDOG_TICK_S=5

# Hard maximum connection duration (seconds). Default is 5400 (90 minutes).
# Set WS_MAX_CONNECTION_DURATION_S=0 to disable max duration close.
WS_MAX_CONNECTION_DURATION_S=5400

# Per-connection inbound message queue size.
WS_INBOUND_QUEUE_MAX=256

# -----------------------------------------------------------------------------
# Rate Limits (per connection)
# -----------------------------------------------------------------------------

# WebSocket message limits.
WS_MESSAGE_WINDOW_SECONDS=60
WS_MAX_MESSAGES_PER_WINDOW=5000

# Cancel limits. If WS_CANCEL_WINDOW_SECONDS is <= 0 or unset, it falls back to
# WS_MESSAGE_WINDOW_SECONDS.
WS_CANCEL_WINDOW_SECONDS=60
WS_MAX_CANCELS_PER_WINDOW=50

# -----------------------------------------------------------------------------
# vLLM Engine
# -----------------------------------------------------------------------------

VLLM_DTYPE=bfloat16
VLLM_GPU_MEMORY_UTILIZATION=0.92

# Voxtral realtime runs at ~80ms/token.
# Default max_model_len is intentionally small to maximize concurrency.
# This repo supports long-running streams by internally rolling segments.
VLLM_MAX_MODEL_LEN=1024

# If unset, the server may auto-tune based on GPU memory and model size.
# VLLM_MAX_NUM_SEQS=128

VLLM_ENFORCE_EAGER=0

# KV cache dtype (vLLM-dependent).
# Default: auto-select FP8 KV cache on FP8-capable GPUs (L40/L40S/H100/etc),
# otherwise fallback to unquantized KV ("auto").
# VLLM_KV_CACHE_DTYPE=fp8

# When using FP8 KV, enable dynamic scale calculation by default.
# VLLM_CALCULATE_KV_SCALES=1

# Voxtral requires Mistral loader/tokenizer settings; these are fixed in code
# and are intentionally not configurable via env.

# Optional JSON dict, or "null" to disable compilation config.
# When exporting in a shell, wrap JSON values in single quotes.
VLLM_COMPILATION_CONFIG={"cudagraph_mode":"PIECEWISE"}

# 1 disables the vLLM compile cache; 0 enables it.
VLLM_DISABLE_COMPILE_CACHE=1

# -----------------------------------------------------------------------------
# Streaming
# -----------------------------------------------------------------------------

# Internal rolling keeps a single client utterance running indefinitely by
# chunking it into bounded vLLM segments (with overlap).
STT_INTERNAL_ROLL=1
STT_SEGMENT_SECONDS=60
STT_SEGMENT_OVERLAP_SECONDS=0.8

# Overload policy: stay live by dropping oldest inbound audio when backlog grows.
STT_MAX_BACKLOG_SECONDS=5

# -----------------------------------------------------------------------------
# Launcher / Dependency Install
# -----------------------------------------------------------------------------

# Torch wheel index used by scripts/steps/04-install-deps.sh for CUDA 12.8.
PYTORCH_CUDA_INDEX_URL=https://download.pytorch.org/whl/cu128
