# Voxtral STT API - Environment Configuration
#
# This file is a template. Use one of:
# - Copy to `.env` and export variables from it in your shell, or
# - Export variables directly in your environment.
#
# Do not commit `.env` (it may contain secrets).

# -----------------------------------------------------------------------------
# Required
# -----------------------------------------------------------------------------

# API key required by all WebSocket clients.
# Clients can pass it as:
# - Query parameter:  ?api_key=...
# - Header:           X-API-Key: ...
VOXTRAL_API_KEY=secret_token

# -----------------------------------------------------------------------------
# Recommended
# -----------------------------------------------------------------------------

# Hugging Face token for model downloads (optional but recommended to avoid
# rate limits).
# HF_TOKEN=hf_...

# -----------------------------------------------------------------------------
# Server Bind / Logging
# -----------------------------------------------------------------------------

# Host/port used by the launcher scripts (scripts/main.sh).
SERVER_BIND_HOST=0.0.0.0
SERVER_PORT=8000

# Logging level for the Python server.
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# Model
# -----------------------------------------------------------------------------

# Voxtral Realtime model id (Hugging Face repo).
VOXTRAL_MODEL_ID=mistralai/Voxtral-Mini-4B-Realtime-2602

# Optional: the model name exposed in the realtime protocol ("model" field).
# Defaults to VOXTRAL_MODEL_ID if not set.
# VOXTRAL_SERVED_MODEL_NAME=mistralai/Voxtral-Mini-4B-Realtime-2602

# Intentional transcription delay (ms). Must be a multiple of 80 between 80 and 2400.
VOXTRAL_TRANSCRIPTION_DELAY_MS=400

# Writable local snapshot directory. The server patches `tekken.json` here.
VOXTRAL_MODEL_DIR=models/voxtral

# Tekken config filename inside the snapshot directory.
VOXTRAL_TEKKEN_FILENAME=tekken.json

# -----------------------------------------------------------------------------
# Connection Limits / Lifecycle
# -----------------------------------------------------------------------------

# Maximum concurrent WebSocket connections.
MAX_CONCURRENT_CONNECTIONS=100

# Idle close (seconds) and watchdog tick interval (seconds).
WS_IDLE_TIMEOUT_S=150
WS_WATCHDOG_TICK_S=5

# Hard maximum connection duration (seconds). Default is 5400 (90 minutes).
WS_MAX_CONNECTION_DURATION_S=5400

# Per-connection inbound message queue size.
WS_INBOUND_QUEUE_MAX=256

# -----------------------------------------------------------------------------
# Rate Limits (per connection)
# -----------------------------------------------------------------------------

# WebSocket message limits.
WS_MESSAGE_WINDOW_SECONDS=60
WS_MAX_MESSAGES_PER_WINDOW=5000

# Cancel limits. If WS_CANCEL_WINDOW_SECONDS is <= 0 or unset, it falls back to
# WS_MESSAGE_WINDOW_SECONDS.
WS_CANCEL_WINDOW_SECONDS=60
WS_MAX_CANCELS_PER_WINDOW=50

# -----------------------------------------------------------------------------
# vLLM Engine
# -----------------------------------------------------------------------------

VLLM_DTYPE=bfloat16
VLLM_GPU_MEMORY_UTILIZATION=0.92

# Voxtral realtime runs at ~80ms/token. 90 minutes ~= 67500 tokens.
VLLM_MAX_MODEL_LEN=67500

VLLM_MAX_NUM_SEQS=128
VLLM_MAX_NUM_BATCHED_TOKENS=4096

VLLM_ENFORCE_EAGER=0

# KV cache dtype: auto, fp8_e4m3fn, fp8_e5m2, etc. (vLLM-dependent).
VLLM_KV_CACHE_DTYPE=auto

# Mistral model loading flags used by vLLM.
VLLM_TOKENIZER_MODE=mistral
VLLM_CONFIG_FORMAT=mistral
VLLM_LOAD_FORMAT=mistral

# Optional JSON dict, or "null" to disable compilation config.
# When exporting in a shell, wrap JSON values in single quotes.
VLLM_COMPILATION_CONFIG={"cudagraph_mode":"PIECEWISE"}

# 1 disables the vLLM compile cache; 0 enables it.
VLLM_DISABLE_COMPILE_CACHE=1

# -----------------------------------------------------------------------------
# Launcher / Dependency Install
# -----------------------------------------------------------------------------

# Torch wheel index used by scripts/steps/03_install_deps.sh for CUDA 13.
PYTORCH_CUDA_INDEX_URL=https://download.pytorch.org/whl/cu130

