"""vLLM engine configuration (constants only)."""

from __future__ import annotations

from typing import Any

ENV_VLLM_DTYPE = "VLLM_DTYPE"
DEFAULT_VLLM_DTYPE = "bfloat16"

ENV_VLLM_GPU_MEMORY_UTILIZATION = "VLLM_GPU_MEMORY_UTILIZATION"
DEFAULT_VLLM_GPU_MEMORY_UTILIZATION = 0.92

# Voxtral Realtime is ~12.5 "steps"/sec (~80ms/token). Default max connection duration is 90 minutes:
# 90min = 5400s; 5400 / 0.08 = 67500 tokens.
ENV_VLLM_MAX_MODEL_LEN = "VLLM_MAX_MODEL_LEN"
DEFAULT_VLLM_MAX_MODEL_LEN = 67500

ENV_VLLM_MAX_NUM_SEQS = "VLLM_MAX_NUM_SEQS"
DEFAULT_VLLM_MAX_NUM_SEQS = 128

ENV_VLLM_MAX_NUM_BATCHED_TOKENS = "VLLM_MAX_NUM_BATCHED_TOKENS"
DEFAULT_VLLM_MAX_NUM_BATCHED_TOKENS = 4096

ENV_VLLM_ENFORCE_EAGER = "VLLM_ENFORCE_EAGER"
DEFAULT_VLLM_ENFORCE_EAGER = False

# KV cache dtype: "auto", "fp8_e4m3fn", "fp8_e5m2", etc. (vLLM-dependent).
ENV_VLLM_KV_CACHE_DTYPE = "VLLM_KV_CACHE_DTYPE"
DEFAULT_VLLM_KV_CACHE_DTYPE = "auto"

# Voxtral recommends Mistral-specific loading flags for vLLM:
#   --tokenizer-mode mistral --config-format mistral --load-format mistral
ENV_VLLM_TOKENIZER_MODE = "VLLM_TOKENIZER_MODE"
DEFAULT_VLLM_TOKENIZER_MODE = "mistral"

ENV_VLLM_CONFIG_FORMAT = "VLLM_CONFIG_FORMAT"
DEFAULT_VLLM_CONFIG_FORMAT = "mistral"

ENV_VLLM_LOAD_FORMAT = "VLLM_LOAD_FORMAT"
DEFAULT_VLLM_LOAD_FORMAT = "mistral"

# Optional JSON string for vLLM compilation config.
#
# Voxtral realtime may require disabling full CUDA graphs in vLLM. Keep the default conservative.
ENV_VLLM_COMPILATION_CONFIG = "VLLM_COMPILATION_CONFIG"
DEFAULT_VLLM_COMPILATION_CONFIG: dict[str, Any] | None = {"cudagraph_mode": "PIECEWISE"}

ENV_VLLM_DISABLE_COMPILE_CACHE = "VLLM_DISABLE_COMPILE_CACHE"
DEFAULT_VLLM_DISABLE_COMPILE_CACHE = True

__all__ = [
    "DEFAULT_VLLM_COMPILATION_CONFIG",
    "DEFAULT_VLLM_CONFIG_FORMAT",
    "DEFAULT_VLLM_DISABLE_COMPILE_CACHE",
    "DEFAULT_VLLM_DTYPE",
    "DEFAULT_VLLM_ENFORCE_EAGER",
    "DEFAULT_VLLM_GPU_MEMORY_UTILIZATION",
    "DEFAULT_VLLM_KV_CACHE_DTYPE",
    "DEFAULT_VLLM_LOAD_FORMAT",
    "DEFAULT_VLLM_MAX_MODEL_LEN",
    "DEFAULT_VLLM_MAX_NUM_BATCHED_TOKENS",
    "DEFAULT_VLLM_MAX_NUM_SEQS",
    "DEFAULT_VLLM_TOKENIZER_MODE",
    "ENV_VLLM_COMPILATION_CONFIG",
    "ENV_VLLM_CONFIG_FORMAT",
    "ENV_VLLM_DISABLE_COMPILE_CACHE",
    "ENV_VLLM_DTYPE",
    "ENV_VLLM_ENFORCE_EAGER",
    "ENV_VLLM_GPU_MEMORY_UTILIZATION",
    "ENV_VLLM_KV_CACHE_DTYPE",
    "ENV_VLLM_LOAD_FORMAT",
    "ENV_VLLM_MAX_MODEL_LEN",
    "ENV_VLLM_MAX_NUM_BATCHED_TOKENS",
    "ENV_VLLM_MAX_NUM_SEQS",
    "ENV_VLLM_TOKENIZER_MODE",
]
